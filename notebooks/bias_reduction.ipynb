{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with Bias and Fairness in Data Science Systems\n",
    "## KDD 2020 Hands-on Tutorial\n",
    "### Pedro Saleiro, Kit Rodolfa, Rayid Ghani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Exploring Bias Reduction Strategies</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies, import packages and data\n",
    "This is needed every time you open this notebook in **colab** to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aequitas in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/aequitas-0.38.1-py3.7.egg (0.38.1)\n",
      "Requirement already satisfied: ohio>=0.2.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/ohio-0.5.0-py3.7.egg (from aequitas) (0.5.0)\n",
      "Requirement already satisfied: Flask==0.12.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/Flask-0.12.2-py3.7.egg (from aequitas) (0.12.2)\n",
      "Requirement already satisfied: Flask-Bootstrap==3.3.7.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/Flask_Bootstrap-3.3.7.1-py3.7.egg (from aequitas) (3.3.7.1)\n",
      "Requirement already satisfied: markdown2==2.3.5 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/markdown2-2.3.5-py3.7.egg (from aequitas) (2.3.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from aequitas) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.24.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from aequitas) (1.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/PyYAML-5.3.1-py3.7-linux-x86_64.egg (from aequitas) (5.3.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.1.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/SQLAlchemy-1.3.19-py3.7-linux-x86_64.egg (from aequitas) (1.3.19)\n",
      "Requirement already satisfied: tabulate==0.8.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/tabulate-0.8.2-py3.7.egg (from aequitas) (0.8.2)\n",
      "Requirement already satisfied: xhtml2pdf==0.2.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/xhtml2pdf-0.2.2-py3.7.egg (from aequitas) (0.2.2)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from aequitas) (0.10.1)\n",
      "Requirement already satisfied: altair==4.1.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/altair-4.1.0-py3.7.egg (from aequitas) (4.1.0)\n",
      "Requirement already satisfied: millify==0.1.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/millify-0.1.1-py3.7.egg (from aequitas) (0.1.1)\n",
      "Requirement already satisfied: Jinja2>=2.4 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from Flask==0.12.2->aequitas) (2.11.2)\n",
      "Requirement already satisfied: Werkzeug>=0.7 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/Werkzeug-1.0.1-py3.7.egg (from Flask==0.12.2->aequitas) (1.0.1)\n",
      "Requirement already satisfied: click>=2.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/click-7.1.2-py3.7.egg (from Flask==0.12.2->aequitas) (7.1.2)\n",
      "Requirement already satisfied: itsdangerous>=0.21 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/itsdangerous-2.0.0a1-py3.7.egg (from Flask==0.12.2->aequitas) (2.0.0a1)\n",
      "Requirement already satisfied: dominate in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/dominate-2.5.1-py3.7.egg (from Flask-Bootstrap==3.3.7.1->aequitas) (2.5.1)\n",
      "Requirement already satisfied: visitor in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/visitor-0.1.3-py3.7.egg (from Flask-Bootstrap==3.3.7.1->aequitas) (0.1.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from matplotlib>=3.0.3->aequitas) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from matplotlib>=3.0.3->aequitas) (1.19.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from matplotlib>=3.0.3->aequitas) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from matplotlib>=3.0.3->aequitas) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from matplotlib>=3.0.3->aequitas) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from pandas>=0.24.1->aequitas) (2020.1)\n",
      "Requirement already satisfied: html5lib>=1.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/html5lib-1.1-py3.7.egg (from xhtml2pdf==0.2.2->aequitas) (1.1)\n",
      "Requirement already satisfied: httplib2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/httplib2-0.18.1-py3.7.egg (from xhtml2pdf==0.2.2->aequitas) (0.18.1)\n",
      "Requirement already satisfied: pyPdf2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/PyPDF2-1.26.0-py3.7.egg (from xhtml2pdf==0.2.2->aequitas) (1.26.0)\n",
      "Requirement already satisfied: Pillow in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from xhtml2pdf==0.2.2->aequitas) (7.2.0)\n",
      "Requirement already satisfied: reportlab>=3.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/reportlab-3.5.48-py3.7-linux-x86_64.egg (from xhtml2pdf==0.2.2->aequitas) (3.5.48)\n",
      "Requirement already satisfied: six in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from xhtml2pdf==0.2.2->aequitas) (1.15.0)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from seaborn>=0.9.0->aequitas) (1.5.2)\n",
      "Requirement already satisfied: entrypoints in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from altair==4.1.0->aequitas) (0.3)\n",
      "Requirement already satisfied: jsonschema in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from altair==4.1.0->aequitas) (3.2.0)\n",
      "Requirement already satisfied: toolz in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages/toolz-0.10.0-py3.7.egg (from altair==4.1.0->aequitas) (0.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from Jinja2>=2.4->Flask==0.12.2->aequitas) (1.1.1)\n",
      "Requirement already satisfied: webencodings in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from html5lib>=1.0->xhtml2pdf==0.2.2->aequitas) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema->altair==4.1.0->aequitas) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema->altair==4.1.0->aequitas) (19.3.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema->altair==4.1.0->aequitas) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from jsonschema->altair==4.1.0->aequitas) (49.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema->altair==4.1.0->aequitas) (3.1.0)\n",
      "Requirement already satisfied: fairlearn in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (0.4.6)\n",
      "Requirement already satisfied: ipywidgets>=7.5.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from fairlearn) (7.5.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from fairlearn) (0.23.2)\n",
      "Requirement already satisfied: pandas>=0.25.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from fairlearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from fairlearn) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from fairlearn) (1.19.1)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipywidgets>=7.5.0->fairlearn) (7.17.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat>=4.2.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipywidgets>=7.5.0->fairlearn) (5.0.7)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipywidgets>=7.5.0->fairlearn) (3.5.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipywidgets>=7.5.0->fairlearn) (4.3.3)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipywidgets>=7.5.0->fairlearn) (5.3.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from scikit-learn>=0.22.1->fairlearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from scikit-learn>=0.22.1->fairlearn) (0.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from pandas>=0.25.1->fairlearn) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from pandas>=0.25.1->fairlearn) (2020.1)\n",
      "Requirement already satisfied: decorator in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (4.4.2)\n",
      "Requirement already satisfied: pygments in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (2.6.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (3.0.6)\n",
      "Requirement already satisfied: jedi>=0.10 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.17.2)\n",
      "Requirement already satisfied: backcall in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.2.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (49.6.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (4.6.3)\n",
      "Requirement already satisfied: ipython-genutils in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (6.1.3)\n",
      "Requirement already satisfied: six in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from traitlets>=4.3.1->ipywidgets>=7.5.0->fairlearn) (1.15.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn) (6.0.4)\n",
      "Requirement already satisfied: jupyter-client in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets>=7.5.0->fairlearn) (6.1.6)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.2.5)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.5.0->fairlearn) (0.7.1)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (0.16.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (1.7.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (19.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.8.3)\n",
      "Requirement already satisfied: jinja2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.11.2)\n",
      "Requirement already satisfied: nbconvert in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (5.6.1)\n",
      "Requirement already satisfied: prometheus-client in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.8.0)\n",
      "Requirement already satisfied: Send2Trash in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.5.0)\n",
      "Requirement already satisfied: argon2-cffi in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (19.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.5.0->fairlearn) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.1.1)\n",
      "Requirement already satisfied: bleach in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (3.1.5)\n",
      "Requirement already satisfied: defusedxml in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.6.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.8.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.4.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.3)\n",
      "Requirement already satisfied: testpath in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.4.4)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (1.14.2)\n",
      "Requirement already satisfied: packaging in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (20.4)\n",
      "Requirement already satisfied: webencodings in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /home/pedro.saleiro/.local/lib/python3.7/site-packages (from cffi>=1.0.0->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.20)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/pedro.saleiro/miniconda3/envs/fairtutorial/lib/python3.7/site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.5.0->fairlearn) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install aequitas\n",
    "!pip install fairlearn\n",
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from aequitas.group import Group\n",
    "from aequitas.bias import Bias\n",
    "from aequitas.fairness import Fairness\n",
    "import aequitas.plot as ap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import fairlearn\n",
    "sns.set() \n",
    "DPI = 200\n",
    "DATAPATH = 'https://github.com/dssg/fairness_tutorial/raw/master/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. \n",
    "\n",
    "As described earlier, the goal here is to select top 1000 project submissions that are likely to not get funded in order to prioritize resource allocation. That corresponds to the metric **Precision at top 1000**.\n",
    "\n",
    "### <font color=red>We audited the \"best\" model at precision at top 1000 and found that it has disparities for True Positive Rate for all attributes that we care about (poverty_level of the school, sex, and school_location_type)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we want to do now?\n",
    "\n",
    "1. Could we have picked a different model that was similar enough in \"precision at top 1000\" but less biased?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load predictions, labels, and attributes for all models that were built to audit\n",
    "\n",
    "We have trained 400 models using random sampling of hyperparameters for the following algorithms: RandomForest, Logistic Regression, MLP and LightGBM. The `evals_df` contains a table with the performance metrics for each model on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals_df = pd.read_csv(DATAPATH +'split2_evals.csv.gz', compression='gzip')\n",
    "# Let's sort the models by Precision at top 1000 predicted positives (our performance metric of interest for this case study)\n",
    "evals_df.sort_values('model_precision', ascending = False)\n",
    "evals_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spot-check the model with highest precision at 1000 to see what type it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Highest precision model classpath and hyperparameters: \\n\\n' , evals_df['model_classpath'][0], '\\n', evals_df['hyperparameters'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed Aequitas audit results \n",
    "\n",
    "The `aequitas_df` contains a table with the bias audit results for the 400 models for all the attributes we care about (we pre-selected the metrics of interest for this tutorial). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aequitas_df = pd.read_csv(DATAPATH + 'split2_aequitas.csv.gz', compression='gzip')\n",
    "aequitas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the `evals_df` with the `aequitas_df` allow us to get a clearer picture how the different models compare with each other in both performance and bias of our groups of interest. \n",
    "\n",
    "The `create_scatter_disparity_performance` method will help us to easily plot the 400 models bias-performance tradeoffs and it's going to be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scatter_disparity_performance(evals_df, aequitas_df,  attr_col, group_name, \n",
    "                                         performance_col='model_precision', bias_metric='tpr', flip_disparity=False, \n",
    "                                         mitigated_tag=None, mitigated_bdf=pd.DataFrame(), mitigated_performance=None, ylim=None):\n",
    "    disparity_df = aequitas_df.loc[(aequitas_df['attribute_name']==attr_col) & (aequitas_df['attribute_value']==group_name)].copy()\n",
    "    disparity_metric = bias_metric + '_disparity'\n",
    "    scatter_schema = ['model_uuid', performance_col, 'attribute_name', 'attribute_value', bias_metric, disparity_metric, 'model_tag']\n",
    "    if flip_disparity:\n",
    "        disparity_df[disparity_metric]= disparity_df.apply(lambda x: 1/x[disparity_metric] , axis=1)\n",
    "    scatter = pd.merge(evals_df, disparity_df, how='left', on=['model_uuid'], left_index=True, sort=True, copy=True)\n",
    "    scatter = scatter[['model_uuid', performance_col, 'attribute_name', 'attribute_value', bias_metric, disparity_metric]].copy()\n",
    "    scatter['model_tag'] = 'Other Models'\n",
    "    scatter.sort_values('model_precision', ascending = False, inplace=True, ignore_index=True)\n",
    "    scatter['model_tag'] = scatter.apply(lambda x: 'Highest Precision at 1000' if int(x.name) < 1 else x['model_tag'], axis=1)\n",
    "    if not mitigated_bdf.empty and mitigated_performance !=None:\n",
    "      mitigated_bdf[performance_col] = mitigated_performance\n",
    "      mitigated_bdf['model_tag'] = mitigated_tag\n",
    "      new_disparity_df = mitigated_bdf.loc[(mitigated_bdf['attribute_name']==attr_col) & (mitigated_bdf['attribute_value']==group_name)].copy()\n",
    "      if flip_disparity:\n",
    "        new_disparity_df[disparity_metric]= new_disparity_df.apply(lambda x: 1/x[disparity_metric] , axis=1)\n",
    "      scatter_new = new_disparity_df[[c for c in new_disparity_df.columns if c in scatter_schema]]\n",
    "      scatter_final = pd.concat([scatter, scatter_new], axis=0)\n",
    "    else:\n",
    "      scatter_final = scatter.copy()\n",
    "    ax = sns.scatterplot(\n",
    "        x='model_precision', y=disparity_metric, hue='model_tag',\n",
    "        data=scatter_final,\n",
    "        alpha=0.5, s=20,\n",
    "    )\n",
    "    if ylim:\n",
    "        plt.ylim(0, 10)\n",
    "    flip_placeholder = 'Flipped' if flip_disparity else ''\n",
    "    ax.set_title('{} {} vs.{} for {}:{}'.format(flip_placeholder, disparity_metric, performance_col, attr_col,group_name ), y=1.)\n",
    "    plt.gcf().set_size_inches((4, 3))\n",
    "    plt.legend(loc='upper left', fontsize='xx-small')\n",
    "\n",
    "    plt.gcf().set_dpi(DPI)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "## Let's see if we could have picked a better model for fairness in Poverty Level\n",
    "\n",
    "Each point in the scatterplot represents a model. We highlight the model we picked before (highest global performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'poverty_level', 'highest', flip_disparity=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we could have picked a better model for fairness in Metro Type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'metro_type', 'urban', flip_disparity=True, ylim=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see if we could have picked a better model for fairness in Teacher sex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_scatter_disparity_performance(evals_df, aequitas_df, 'teacher_sex', 'female', flip_disparity=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Reduction Strategies\n",
    "We will try a few different bias reduction strategies now and compare the audit results with the original models\n",
    "\n",
    "1. Re-Sampling\n",
    "2. Regularization (using Fairlearn package)\n",
    "3. Post-hoc adjustment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">Bias Reduction Strategy 1: Re-Sampling</font>\n",
    "### Can resampling approaches help improve the fairness of our models?\n",
    "\n",
    "1. Load data\n",
    "2. Look at training data distributions\n",
    "3. Try Resampling in a few different ways\n",
    "4. Rebuild model(s) on resampled training data\n",
    "5. Predict on the test set\n",
    "6. Audit for Bias and Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a methods based on resampling to reduce this bias in the selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(DATAPATH + 'train_20120501_20120801.csv.gz', compression='gzip')\n",
    "testdf = pd.read_csv(DATAPATH + 'test_20121201_20130201.csv.gz', compression='gzip')\n",
    "train_attrdf = pd.read_csv(DATAPATH + 'train_20120501_20120801_protected.csv.gz', compression='gzip')\n",
    "test_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16790, 113)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-built models and predictions\n",
    "\n",
    "The `evals_df` contains a table with the performance metrics for each model on the holdout test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_precision</th>\n",
       "      <th>model_classpath</th>\n",
       "      <th>hyperparameters</th>\n",
       "      <th>model_uuid</th>\n",
       "      <th>predictions_uuid</th>\n",
       "      <th>target_pp</th>\n",
       "      <th>matrix_type</th>\n",
       "      <th>matrix_start_date</th>\n",
       "      <th>matrix_end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.552</td>\n",
       "      <td>sklearn.ensemble.RandomForestClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...</td>\n",
       "      <td>a04e2eedd9c5ff18bcf77e84ae9db561</td>\n",
       "      <td>c598fbe93f4c218ac7d325fb478598f1</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.551</td>\n",
       "      <td>sklearn.ensemble.RandomForestClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...</td>\n",
       "      <td>3f8f65db414b74ba1c4773e131bdb51d</td>\n",
       "      <td>2d8e4d82f4e67e1de1de8650a7838fc2</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.550</td>\n",
       "      <td>sklearn.ensemble.RandomForestClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...</td>\n",
       "      <td>8823d053f9fbe6c34efdae3a2f24c39c</td>\n",
       "      <td>5e78e49f70fac0193eaadb8513428f23</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.549</td>\n",
       "      <td>lightgbm.LGBMClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"max_depth\": 2, \"num_leaves\": 5...</td>\n",
       "      <td>ffc7abef4fe933b0f0cfed63d5d03910</td>\n",
       "      <td>f04edad1446642411085e84271898506</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.548</td>\n",
       "      <td>sklearn.ensemble.RandomForestClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...</td>\n",
       "      <td>110d892ce37127a79d7c6e0eac9a5d13</td>\n",
       "      <td>09092f371176a13cddca59bac20da4fd</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.373</td>\n",
       "      <td>sklearn.neural_network.MLPClassifier</td>\n",
       "      <td>{\"alpha\": 0.00009627719435096597, \"beta_1\": 0....</td>\n",
       "      <td>b52e7837725a5a34544c10297256fc81</td>\n",
       "      <td>6020b409648f794f6f55ed235924afe3</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.372</td>\n",
       "      <td>sklearn.neural_network.MLPClassifier</td>\n",
       "      <td>{\"alpha\": 0.000022779312051721987, \"beta_1\": 0...</td>\n",
       "      <td>e79173c1595bc2ba28edee0c01467fa9</td>\n",
       "      <td>b16ad6b2a7eba2e6ba30c14981ebaf3b</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.371</td>\n",
       "      <td>sklearn.neural_network.MLPClassifier</td>\n",
       "      <td>{\"alpha\": 0.0000629250653559928, \"beta_1\": 0.9...</td>\n",
       "      <td>5423f461c575db9934e52b49c2aabf0f</td>\n",
       "      <td>ca1c9b8e742db3761a660cac81a19102</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.364</td>\n",
       "      <td>lightgbm.LGBMClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"max_depth\": 14, \"num_leaves\": ...</td>\n",
       "      <td>853e04831b0a855f22f4bd46b995223c</td>\n",
       "      <td>3e9271171981a27f976a9ef8c739cad0</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.318</td>\n",
       "      <td>lightgbm.LGBMClassifier</td>\n",
       "      <td>{\"n_jobs\": -1, \"max_depth\": 102, \"num_leaves\":...</td>\n",
       "      <td>a2190820ccd19bb46ef4c97d880a36f6</td>\n",
       "      <td>b99e776969120280d81984b926373c70</td>\n",
       "      <td>1000</td>\n",
       "      <td>test</td>\n",
       "      <td>2012-12-01</td>\n",
       "      <td>2013-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     model_precision                          model_classpath  \\\n",
       "0              0.552  sklearn.ensemble.RandomForestClassifier   \n",
       "1              0.551  sklearn.ensemble.RandomForestClassifier   \n",
       "2              0.550  sklearn.ensemble.RandomForestClassifier   \n",
       "3              0.549                  lightgbm.LGBMClassifier   \n",
       "4              0.548  sklearn.ensemble.RandomForestClassifier   \n",
       "..               ...                                      ...   \n",
       "395            0.373     sklearn.neural_network.MLPClassifier   \n",
       "396            0.372     sklearn.neural_network.MLPClassifier   \n",
       "397            0.371     sklearn.neural_network.MLPClassifier   \n",
       "398            0.364                  lightgbm.LGBMClassifier   \n",
       "399            0.318                  lightgbm.LGBMClassifier   \n",
       "\n",
       "                                       hyperparameters  \\\n",
       "0    {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...   \n",
       "1    {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...   \n",
       "2    {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...   \n",
       "3    {\"n_jobs\": -1, \"max_depth\": 2, \"num_leaves\": 5...   \n",
       "4    {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth...   \n",
       "..                                                 ...   \n",
       "395  {\"alpha\": 0.00009627719435096597, \"beta_1\": 0....   \n",
       "396  {\"alpha\": 0.000022779312051721987, \"beta_1\": 0...   \n",
       "397  {\"alpha\": 0.0000629250653559928, \"beta_1\": 0.9...   \n",
       "398  {\"n_jobs\": -1, \"max_depth\": 14, \"num_leaves\": ...   \n",
       "399  {\"n_jobs\": -1, \"max_depth\": 102, \"num_leaves\":...   \n",
       "\n",
       "                           model_uuid                  predictions_uuid  \\\n",
       "0    a04e2eedd9c5ff18bcf77e84ae9db561  c598fbe93f4c218ac7d325fb478598f1   \n",
       "1    3f8f65db414b74ba1c4773e131bdb51d  2d8e4d82f4e67e1de1de8650a7838fc2   \n",
       "2    8823d053f9fbe6c34efdae3a2f24c39c  5e78e49f70fac0193eaadb8513428f23   \n",
       "3    ffc7abef4fe933b0f0cfed63d5d03910  f04edad1446642411085e84271898506   \n",
       "4    110d892ce37127a79d7c6e0eac9a5d13  09092f371176a13cddca59bac20da4fd   \n",
       "..                                ...                               ...   \n",
       "395  b52e7837725a5a34544c10297256fc81  6020b409648f794f6f55ed235924afe3   \n",
       "396  e79173c1595bc2ba28edee0c01467fa9  b16ad6b2a7eba2e6ba30c14981ebaf3b   \n",
       "397  5423f461c575db9934e52b49c2aabf0f  ca1c9b8e742db3761a660cac81a19102   \n",
       "398  853e04831b0a855f22f4bd46b995223c  3e9271171981a27f976a9ef8c739cad0   \n",
       "399  a2190820ccd19bb46ef4c97d880a36f6  b99e776969120280d81984b926373c70   \n",
       "\n",
       "     target_pp matrix_type matrix_start_date matrix_end_date  \n",
       "0         1000        test        2012-12-01      2013-01-31  \n",
       "1         1000        test        2012-12-01      2013-01-31  \n",
       "2         1000        test        2012-12-01      2013-01-31  \n",
       "3         1000        test        2012-12-01      2013-01-31  \n",
       "4         1000        test        2012-12-01      2013-01-31  \n",
       "..         ...         ...               ...             ...  \n",
       "395       1000        test        2012-12-01      2013-01-31  \n",
       "396       1000        test        2012-12-01      2013-01-31  \n",
       "397       1000        test        2012-12-01      2013-01-31  \n",
       "398       1000        test        2012-12-01      2013-01-31  \n",
       "399       1000        test        2012-12-01      2013-01-31  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals_df = pd.read_csv(DATAPATH +'split2_evals.csv.gz', compression='gzip')\n",
    "# Let's sort the models by Precision at top 1000 predicted positives (our performance metric of interest for this case study)\n",
    "evals_df.sort_values('model_precision', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the \"Best\" performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest precision model classpath and hyperparameters: \n",
      "\n",
      " sklearn.ensemble.RandomForestClassifier \n",
      " {\"n_jobs\": -1, \"criterion\": \"gini\", \"max_depth\": 30, \"max_features\": \"sqrt\", \"n_estimators\": 87, \"random_state\": 213500298, \"min_samples_leaf\": 44, \"min_samples_split\": 3}\n"
     ]
    }
   ],
   "source": [
    "print('Highest precision model classpath and hyperparameters: \\n\\n' , evals_df['model_classpath'][0], '\\n', evals_df['hyperparameters'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's load the hyperparameters of the highest performance model to a dictionary\n",
    "import ast\n",
    "hyperparameters= ast.literal_eval(evals_df['hyperparameters'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model with highest performance is a RandomForestClassifier let's create a classifier with the fetched hyperparameters\n",
    "rf = RandomForestClassifier(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': DecisionTreeClassifier(),\n",
       " 'n_estimators': 87,\n",
       " 'estimator_params': ('criterion',\n",
       "  'max_depth',\n",
       "  'min_samples_split',\n",
       "  'min_samples_leaf',\n",
       "  'min_weight_fraction_leaf',\n",
       "  'max_features',\n",
       "  'max_leaf_nodes',\n",
       "  'min_impurity_decrease',\n",
       "  'min_impurity_split',\n",
       "  'random_state',\n",
       "  'ccp_alpha'),\n",
       " 'bootstrap': True,\n",
       " 'oob_score': False,\n",
       " 'n_jobs': -1,\n",
       " 'random_state': 213500298,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False,\n",
       " 'class_weight': None,\n",
       " 'max_samples': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': 30,\n",
       " 'min_samples_split': 3,\n",
       " 'min_samples_leaf': 44,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_impurity_split': None,\n",
       " 'ccp_alpha': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining label, group of interest and reference group for the attribute column we care about\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'quickstart_label'\n",
    "attribute_col = 'poverty_level'\n",
    "\n",
    "group_of_interest = 'highest'\n",
    "reference_group = 'lower'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at training data distributions\n",
    "\n",
    "#### Poverty_level=Highest\n",
    "label_pos_poverty_highest =  P(poverty_level=highest | not_funded)\n",
    "\n",
    "label_neg_poverty_highest =  P(poverty_level=highest | funded)\n",
    "\n",
    "\n",
    "#### Poverty_level=Lower\n",
    "label_pos_poverty_lower =  P(poverty_level=lower | not_funded)\n",
    "\n",
    "label_neg_poverty_lower =  P(poverty_level=lower | funded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "highest    9448\n",
       "lower      7342\n",
       "Name: poverty_level, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attrdf[attribute_col].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos_group_of_interest = traindf.loc[(train_attrdf[attribute_col]==group_of_interest) & (traindf[label_col] > 0)]\n",
    "label_neg_group_of_interest = traindf.loc[(train_attrdf[attribute_col]==group_of_interest) & (traindf[label_col] < 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_pos_reference_group = traindf.loc[(train_attrdf[attribute_col]==reference_group) & (traindf[label_col] > 0)]\n",
    "label_neg_reference_group = traindf.loc[(train_attrdf[attribute_col]==reference_group) & (traindf[label_col] < 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3196, 113)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pos_group_of_interest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6252, 113)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_neg_group_of_interest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3130, 113)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pos_reference_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4212, 113)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_neg_reference_group.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest poverty_level training set prevalence: 0.338272650296359\n"
     ]
    }
   ],
   "source": [
    "print('{} {} training set prevalence:'.format(group_of_interest, attribute_col), len(label_pos_group_of_interest) / len(train_attrdf[train_attrdf[attribute_col]==group_of_interest]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower poverty_level training set prevalence: 0.42631435576137294\n"
     ]
    }
   ],
   "source": [
    "print('{} {} training set prevalence:'.format(reference_group, attribute_col), len(label_pos_reference_group) / len(train_attrdf[train_attrdf[attribute_col]==reference_group]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What type of disparities do we see in the data distribution here?\n",
    "\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now try resampling\n",
    "\n",
    "We can perform three types of resampling:\n",
    "\n",
    "1. Change the training data such that different poverty levels are distributed more uniformly but keep the distribution of labels the same within each poverty level P(poverty_level = highest) = P (poverty_level=lower)\n",
    "\n",
    "\n",
    "2. Change the training data such that different poverty levels have more uniform label distributions P(poverty_level = highest | not funded ) = P(poverty_level=lower | not funded)\n",
    "\n",
    "\n",
    "3. Change both\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Change the training data such that different poverty levels have more uniform label distributions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recapping the group of interest and reference we defined above: \n",
      "\n",
      " Attribute: poverty_level\n",
      " Group of interest: highest\n",
      " Reference Group: lower\n"
     ]
    }
   ],
   "source": [
    "print('Recapping the group of interest and reference we defined above: \\n\\n Attribute: {}\\n Group of interest: {}\\n Reference Group: {}'.format(attribute_col, group_of_interest, reference_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest new training prevalence: 0.38650306748466257\n"
     ]
    }
   ],
   "source": [
    "n_pos_group_of_interest = 3150\n",
    "n_neg_group_of_interest = 5000\n",
    "print('{} new training prevalence:'.format(group_of_interest), n_pos_group_of_interest / (n_pos_group_of_interest + n_neg_group_of_interest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower new training prevalence: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "n_pos_reference_group = 2500\n",
    "n_neg_reference_group  = 4000\n",
    "print('{} new training prevalence:'.format(reference_group), n_pos_reference_group / (n_pos_reference_group + n_neg_reference_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pos_group_of_interest = label_pos_group_of_interest.sample(n=n_pos_group_of_interest, replace=False)\n",
    "sample_neg_group_of_interest = label_neg_group_of_interest.sample(n=n_neg_group_of_interest, replace=False)\n",
    "\n",
    "sample_pos_reference_group = label_pos_reference_group.sample(n=n_pos_reference_group, replace=False)\n",
    "sample_neg_reference_group = label_neg_reference_group.sample(n=n_neg_reference_group, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild model on resampled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, max_features='sqrt', min_samples_leaf=44,\n",
       "                       min_samples_split=3, n_estimators=87, n_jobs=-1,\n",
       "                       random_state=213500298)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_traindf = pd.concat([sample_pos_group_of_interest,sample_neg_group_of_interest,sample_pos_reference_group, sample_neg_reference_group], axis=0)\n",
    "y_train = new_traindf[label_col].values\n",
    "rf.fit(new_traindf.drop(['entity_id','as_of_date',label_col], axis = 1), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict on the test set and calculate precision at 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Precision:  0.564\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf.predict_proba(testdf.drop(['entity_id','as_of_date',label_col], axis = 1))[:,1]\n",
    "new_preds = testdf[['entity_id','as_of_date',label_col]].copy()\n",
    "new_preds['predict_proba'] = y_pred\n",
    "new_preds = new_preds.sort_values('predict_proba', ascending = False).reset_index(drop=True).copy()\n",
    "new_preds['score'] = new_preds.apply(lambda x: 1.0 if int(x.name)  < 1000 else 0.0, axis=1)\n",
    "print('Model Precision: ', new_preds[new_preds['score'] > 0][label_col].sum() / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before and After: Run Aequitas\n",
    "\n",
    "For reference, let's start with looking at the \"best\" model we chose in terms of overall precision at 1,000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the predictions from the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col='entity_id'\n",
    "date_col = 'as_of_date'\n",
    "top_k= 1000\n",
    "old_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "old_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "old_df = pd.merge(old_preds, old_attrdf, how='left', on=[id_col, date_col], left_index=True, right_index=False, sort=True, copy=True)\n",
    "\n",
    "old_df = old_df.sort_values('predict_proba', ascending=False)\n",
    "old_df = old_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "old_df['score'] = old_df.apply(lambda x: 1.0 if x.name in old_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before: Run Aequitas for the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id, score_thresholds 0 {'rank_abs': [1000]}\n",
      "get_disparity_predefined_group()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'disparity_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-edced72940cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbdf_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_disparity_predefined_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtab_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_groups_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'poverty_level'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'lower'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metro_type'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'suburban_rural'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'teacher_sex'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisparities_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbdf_old\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfairness_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisparity_threshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'disparity_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "metrics = ['tpr']\n",
    "xtab_old, _ = g.get_crosstabs(old_df[['score','label_value','poverty_level','metro_type', 'teacher_sex']].copy())\n",
    "bdf_old = b.get_disparity_predefined_groups(xtab_old, original_df=old_df, ref_groups_dict={'poverty_level':'lower', 'metro_type':'suburban_rural', 'teacher_sex':'male'})\n",
    "\n",
    "ap.disparities_metrics(bdf_old, metrics, attribute_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.disparities(bdf_old, metrics, 'poverty_level', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After: Audit for Bias for the new mitigated model  (keeping the attributes, reference groups, bias metric, and tolerance the same as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(new_preds, test_attrdf, how='left', on=['entity_id','as_of_date'], left_index=True, right_index=False, sort=True, copy=True)\n",
    "df = df.rename(columns = {label_col:'label_value'})\n",
    "metrics = ['tpr']\n",
    "g = Group()\n",
    "xtab, _ = g.get_crosstabs(df[['score','label_value','poverty_level','metro_type', 'teacher_sex']].copy())\n",
    "b = Bias()\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, \n",
    "                                        ref_groups_dict={'poverty_level':'lower', 'metro_type':'suburban_rural', 'teacher_sex':'male'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap.disparities(bdf, metrics, 'poverty_level', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap.disparities(bdf, metrics, 'metro_type', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ap.disparities(bdf, metrics, 'teacher_sex', fairness_threshold = 1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also look at the raw metrics to see what's changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_metrics = g.list_absolute_metrics(xtab)\n",
    "xtab[['attribute_name', 'attribute_value'] + absolute_metrics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.loc[old_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the new, fairness-aware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the model selection (tradeoff) graph\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = df.loc[df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Resampling',\n",
    "    'mitigated_bdf':bdf, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Bias Reduction Strategy 2: Regularization (using Fairlearn package)</font>\n",
    "### <font color=red>In-Processing Fairness Improvement</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install dependencies, import packages and data\n",
    "This is needed every time you open this notebook in colab to install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the methods \n",
    "from fairlearn.reductions import ExponentiatedGradient, GridSearch, DemographicParity, TruePositiveRateDifference\n",
    "from fairlearn.metrics import selection_rate_group_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a method of using in-processing to train a fairness-aware classifier in order to reduce this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>FairLearn - a reductions approach</font>\n",
    "\n",
    "[Paper](https://arxiv.org/pdf/1803.02453.pdf): _A Reductions Approach to Fair Classification_, 2018\n",
    "\n",
    "> We present a systematic approach for achievingfairness in a binary classification setting. Whilewe focus on two well-known quantitative defini-tions of fairness, our approach encompasses manyother  previously  studied  definitions  as  specialcases. The key idea is to __reduce fair classification__ to a __sequence  of  cost-sensitive__  classification problems, whose solutions yield a randomized classifier with the __lowest (empirical) error__ subject to  the  __desired  constraints__.   We  introduce  two reductions that work for any representation of the cost-sensitive  classifier  and  compare  favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.\n",
    "\n",
    "[FairLearn Documentation](https://fairlearn.github.io/user_guide/mitigation.html#id17)\n",
    "\n",
    "\n",
    "\n",
    "### TLDR; \n",
    "\n",
    "- This approach poses Fair Learning as a constrained optimization problem: minimize the empirical error, subject to linear constraints of the fairness (e.g., TPR difference, demographic parity).\n",
    "- Solve the constrained optimization as a __cost-sensitive__ classification problem.\n",
    "- Obtain a __randomized classifier__, which implies they will create multiple base estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test matrices as well as protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(DATAPATH + 'train_20120501_20120801.csv.gz', compression='gzip')\n",
    "testdf = pd.read_csv(DATAPATH + 'test_20121201_20130201.csv.gz', compression='gzip')\n",
    "train_attrdf = pd.read_csv(DATAPATH + 'train_20120501_20120801_protected.csv.gz', compression='gzip')\n",
    "test_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some parameters we'll need below and create matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'quickstart_label'\n",
    "date_col = 'as_of_date'\n",
    "id_col = 'entity_id'\n",
    "attr_col = 'poverty_level'\n",
    "exclude_cols = [label_col, date_col, id_col]\n",
    "\n",
    "top_k = 1000\n",
    "\n",
    "# aequitas parameters\n",
    "metrics = ['tpr']\n",
    "disparity_threshold = 1.3\n",
    "protected_attribute_ref_group = {attr_col:'lower'}\n",
    "\n",
    "\n",
    "X_train, y_train, A_train = traindf[[c for c in traindf.columns if c not in exclude_cols]].values, traindf[label_col].values, train_attrdf[[attr_col]]\n",
    "X_test,   y_test,   A_test   = testdf[[c for c in testdf.columns if c not in exclude_cols]].values,   testdf[label_col].values  , test_attrdf[[attr_col]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a fairness-improving classifier\n",
    "\n",
    "To account fairness during model training, we'll use the **Exponentiated Gradient** provided by the `fairlearn` module.\n",
    "\n",
    "\n",
    "Its hyperparameters are: \n",
    "- `estimator`: an estimator that implements the methods `fit(X, y, sample_weight)` and `predict(X)`.\n",
    "- `constraints`: fairness constraints.\n",
    "- `eps: float`: fairness threshold, i.e., how much constraint violation we support (defaults to 0.01). \n",
    "- `T: int`: maximum number of iterations (defaults to 50).\n",
    "- `nu: float`: convergence threshold for duality gap (defaults to None).\n",
    "- `eta_0: float`: initial learning rate (defaults to 2).\n",
    "- `run_linprog_step: bool`: whether to apply saddle point optimization to the convex hull of classifiers obtained so far, after each exponentiated gradient step (defaults to True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Exponentiated Gradient has a stoachastic component\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're using `TruePositiveRateDifference` for our fairness constraint here since we care about equalizing **recall** (aka **tpr** aka **equality of opportunity**) across our subgroups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Step 1. Define the constraint\n",
    "constraint = TruePositiveRateDifference()\n",
    "\n",
    "# Step 2. Define the base estimator (any estimator providing 'fit' and 'predict')\n",
    "# Note: we could have used other algorithm such as logistic regression or random forest\n",
    "base_estimator = DecisionTreeClassifier(max_depth=20, min_samples_leaf=10)\n",
    "\n",
    "# Step 3. Define the bias reducer algorithm you want to apply\n",
    "bias_reducer = ExponentiatedGradient(base_estimator, constraint, T=50)\n",
    "\n",
    "# Step 4. Fit the data (and provide the sensitive attributes)\n",
    "bias_reducer.fit(X_train, y_train, sensitive_features=A_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Use the mitigator to make predictions \n",
    "y_pred = bias_reducer.predict(X_test)\n",
    "new_preds = testdf[['entity_id','as_of_date','quickstart_label']].copy()\n",
    "new_preds['score'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at the output\n",
    "Notice that unlike many classifiers, the `ExponentiatedGradient` doesn't have a method for predicting a continuous score, just predicted classes of 0 or 1. How many projects did this model predict are at risk of going unfunded (that is, predicted class of 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_preds['score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like about 6,500 projects are predicted as being at risk of going unfunded by this classifier, but unfortunately our program is resource-constrained and can only help 1,000 of them. At this point, if we wanted to pick out the 1,000 highest-risk projects (subject to our fairness constraint), we're a bit stuck: **the classifier doesn't give us any method for distinguishing higher-risk vs lower-risk projects!**\n",
    "\n",
    "Given this limitation, we might posit that a reasonable approach would be to pick 1,000 projects to intervene with from among these 6,500. Let's see what would happen if we did that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before and After: Run Aequitas\n",
    "\n",
    "For reference, let's start with looking at the \"best\" model we chose in terms of overall precision at 1,000:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the predictions from the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "old_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "old_df = pd.merge(old_preds, old_attrdf, how='left', on=[id_col, date_col], left_index=True, right_index=False, sort=True, copy=True)\n",
    "\n",
    "old_df = old_df.sort_values('predict_proba', ascending=False)\n",
    "old_df = old_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "old_df['score'] = old_df.apply(lambda x: 1.0 if x.name in old_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before: Run Aequitas for the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_old, _ = g.get_crosstabs(old_df[['score', 'label_value', attr_col]].copy())\n",
    "bdf_old = b.get_disparity_predefined_groups(xtab_old, original_df=old_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_old, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After: Run Aequitas for the new, fairness-aware model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember here that we would choose 1,000 at random from the 6,500 with predicted class 1, so the expected value for the recall disparity of this randomly-selected set would just be the value of full set (that is, the recall of each subgroup would, on average, be proportionally lower in the sub-sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(new_preds, test_attrdf, how='left', on=['entity_id','as_of_date'], left_index=True, right_index=False, sort=True, copy=True)\n",
    "df = df.rename(columns = {label_col:'label_value'})\n",
    "\n",
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab, _ = g.get_crosstabs(df[['score','label_value',attr_col]].copy())\n",
    "bdf = b.get_disparity_predefined_groups(xtab, original_df=df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities(bdf, metrics, attr_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good! The new model appears to have reduced the disparity across poverty levels considerably relative to what we saw when training a model without a fairness constraint and choosing based on precision alone.\n",
    "\n",
    "However, the natural question here is where there is a fairness-accuracy trade-off here: What cost did we incur in terms of model performance, that is overall precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the \"best\" model chosen earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df.loc[old_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision of the new, fairness-aware model\n",
    "\n",
    "As above, we would be sampling from the 6,500 down to 1,000 but the expected value of precision in this sample would just be the mean label value in the full population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, compared to the old model, this method has resulted in **quite a large trade-off in model performance to acheive fairness**. This is certainly in part a result of the lack of flexibility in the method not allowing us to provide a score threshold or top k size that we're interested in equalizing our fairness metric around and instead using a built-in threshold that yields 6,500 predicted positives when we're only able to intervene on 1,000. Unfortunately, this sort of inflexibility appears to be a common attribute of many in-processing methods available today.\n",
    "\n",
    "For context, the overall base rate (`df['label_value'].mean()`) is 0.338, so the drop-off in precision here is about half way from our previous model to simply choosing at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to model selection\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = df.loc[df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Fairlearn Reductions',\n",
    "    'mitigated_bdf':bdf, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Bias Reduction Strategy 3: Post-Hoc Disparity Mitigation</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What has already happened?\n",
    "\n",
    "We've already cleaned data, generated features, created train-test sets, built 1000s of models on each training set and scored each test set with them, and calculated various evaluation metrics. We then used these results to pick a \"best\" model in terms of performance on the \"accuracy\" metric we care about: **Precision at the top 1000** (corresponding to our goal of selecting 1000 project submissions that are most likely to not get funded in order to prioritize resource allocation).\n",
    "\n",
    "When we audited this selected model with Aequitas, however, we found biases across many attributes, including the poverty level of the schools. Here, we explore a method of using post-hoc disparity mitigation to reduce this bias in the selected model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=green>Intro to Post-Hoc Bias Mitigation</font>\n",
    "\n",
    "![Diagram of Post-Hoc Adjustments](post_hoc_adj.png \"Post-Hoc Adjustments\")\n",
    "\n",
    "One approach to improving the fairness of our model is to make post-hoc adjustments to the thresholds used for each subgroup to choose the 1,000 projects on which to intervene. Because our fairness metric here (**recall** aka **tpr** aka **equality of opportunity**) is monatonically increasing with the depth of the score, we should be able to find score thresholds for each subgroup that will equalize this metric across the groups, subject to the constraint that we want to choose a total of 1,000 projects for our intervation.\n",
    "\n",
    "In short, here's how this will work (see the references below for a more detailed discussion):\n",
    "1. Train the model as usual on a training set, predict scores on a test set\n",
    "2. Split this test set by subgroups on our protected attribute (here, poverty level)\n",
    "3. Sort each subgroup by score and calculate the cumulative tpr/recall up to and including each row in the set, storing this \"rolling within-subgroup recall\" value\n",
    "4. Recombine the subgroups, and sort the entire set by this new value\n",
    "5. Take the top 1,000 projects from this re-ordered list and use it to calculate \"top k\" sizes for each subgroup that equalize recall\n",
    "6. Then, on a future test set, use these calculated subgroup list sizes to assess the impact of disparities and overall precision\n",
    "\n",
    "References:\n",
    "- Hardt, et al, [Equality of Opportunity in Supervised Learning](http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning)\n",
    "- Rodolfa, et al, [Case Study: Predictive Fairness to Reduce Misdemeanor Recidivism Through Social Service Interventions](https://dl.acm.org/doi/abs/10.1145/3351095.3372863?casa_token=zc196JJrqkkAAAAA:bPmqmKrA91esJhIxHPT4K1crWWb5JGcflVFDkTgODctMzLpUX50_56Kyyh4NJ2GTd_QSydqhNpjT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the train, test, and protected attributes from the first split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_traindf = pd.read_csv(DATAPATH + 'train_20111101_20120201.csv.gz', compression='gzip')\n",
    "split1_testdf = pd.read_csv(DATAPATH + 'test_20120601_20120801.csv.gz', compression='gzip')\n",
    "split1_attrdf = pd.read_csv(DATAPATH + 'test_20120601_20120801_protected.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just take a quick look at the data to make sure it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up some parameters we'll need below\n",
    "\n",
    "Note that the classifier type and hyperparameters here are from the best-performing model we chose above based on precision on the top 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'criterion': 'gini',\n",
    "    'max_depth': 30,\n",
    "    'max_features': 'sqrt',\n",
    "    'min_samples_leaf': 44,\n",
    "    'min_samples_split': 3,\n",
    "    'n_estimators': 87,\n",
    "    'n_jobs': -1,\n",
    "    'random_state': 213500298\n",
    "}\n",
    "clf = RandomForestClassifier(**hyperparameters)\n",
    "\n",
    "top_k = 1000\n",
    "\n",
    "label_col = 'quickstart_label'\n",
    "entity_col = 'entity_id'\n",
    "date_col = 'as_of_date'\n",
    "exclude_cols = [label_col, entity_col, date_col] # columns to exclude from the X matrices for the classifier\n",
    "\n",
    "protected_attribute_col = 'poverty_level'\n",
    "\n",
    "# Parameters for Aequitas\n",
    "\n",
    "metrics = ['tpr']\n",
    "disparity_threshold = 1.3\n",
    "protected_attribute_ref_group = {protected_attribute_col:'lower'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "y_train = split1_traindf[label_col].values\n",
    "clf.fit(split1_traindf.drop(exclude_cols, axis = 1), y_train)\n",
    "\n",
    "# test set predictions\n",
    "split1_preds = split1_testdf[[entity_col, date_col, label_col]].copy()\n",
    "split1_preds['predict_proba'] = clf.predict_proba(split1_testdf.drop(exclude_cols, axis = 1))[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the predictions to make sure they look good:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_preds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine predictions with protected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(split1_preds, split1_attrdf, how='left', on=[entity_col,date_col], left_index=True, right_index=False, sort=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort by score, then split by protected attribute (poverty level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_groups = df[protected_attribute_col].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('predict_proba', ascending=False)\n",
    "subgroup_dfs = []\n",
    "for grp in protected_attribute_groups:\n",
    "    subgroup_dfs.append(df[df[protected_attribute_col]==grp].copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate within-subgroup cumulative recall\n",
    "\n",
    "Here, we calculate the recall up to and including each row within the highest-poverty and lower-poverty subsets of the test set. Doing this allows us to recombine and sort the sets in a way that will let find recall-equalizing \"top k\" list sizes for each subgroup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subgrp_df in subgroup_dfs:\n",
    "    subgrp_df['cumsum_recall'] = subgrp_df[label_col].cumsum() / subgrp_df[label_col].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recombine the subgroup sets and sort by this cumulative recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_df = pd.concat(subgroup_dfs, axis=0).sort_values('cumsum_recall', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find subgroup sizes, holding the overall list size (1000) constant\n",
    "\n",
    "Now we can simply threshold this re-sorted list by `top_k` (here, 1000) to identify how many individuals from each group we should apply in the future.\n",
    "\n",
    "Notice here that each subgroup will still be ordered by their predicted score, but the scores will no longer be perfectly ordered across subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp = recall_df.head(top_k).copy()\n",
    "new_pp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp[protected_attribute_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just store this to re-use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroup_k = {} \n",
    "for grp in protected_attribute_groups:\n",
    "  subgroup_k[grp] = new_pp[protected_attribute_col].value_counts()[grp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply these subgroup-specific sizes to future test set data\n",
    "\n",
    "Now we have calculated the number of projects we need to select from each poverty level, we can apply these to the most recent split to assess how well this method reduces the recall disparities we saw initially and whether this has any impact on the overall precision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the predictions and protected attributes from the future test set\n",
    "\n",
    "Note that the predictions here correspond to the same model + hyperparameters we specified above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2_preds = pd.read_csv(DATAPATH + 'predictions_c598fbe93f4c218ac7d325fb478598f1.csv.gz', compression='gzip')\n",
    "split2_attrdf = pd.read_csv(DATAPATH + 'test_20121201_20130201_protected.csv.gz', compression='gzip')\n",
    "\n",
    "df2 = pd.merge(split2_preds, split2_attrdf, how='left', on=[entity_col, date_col], left_index=True, right_index=False, sort=True, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a quick look to make sure the data loaded without any issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the test set by poverty level to apply the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.sort_values('predict_proba', ascending=False)\n",
    "new_subgroup_dfs = {}\n",
    "for grp in protected_attribute_groups:\n",
    "    new_subgroup_dfs[grp] = df2[df2[protected_attribute_col]==grp].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the number of projects from each subgroup found above\n",
    "\n",
    "Notice here that we're choosing the \"top k\" individuals within each subgroup based on their predicted score -- in a deployment, we wouldn't know the true labels to calculate recall values, which is why we had to go one step back in time to find these group sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_dfs = []\n",
    "for grp in protected_attribute_groups:\n",
    "    pp_dfs.append(new_subgroup_dfs[grp].head(subgroup_k[grp]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recombine and create a predicted class label for this resulting set\n",
    "That is, 1,000 projects with a label 1 chosen by this process and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pp2 = pd.concat(pp_dfs, axis=0).sort_values('predict_proba', ascending=True)\n",
    "\n",
    "mitigated_df = df2.copy()\n",
    "mitigated_df = mitigated_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "mitigated_df['score'] = mitigated_df.apply(lambda x: 1.0 if x.name in new_pp2.index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for comparison, let's also look at the unmitigated result again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unadjusted_df = df2.sort_values('predict_proba', ascending=False).copy()\n",
    "unadjusted_df = unadjusted_df.rename(columns = {label_col:'label_value'}) # naming for Aequitas\n",
    "\n",
    "# create a \"score\" column with the predicted class (named \"score\" for use with Aequitas below)\n",
    "unadjusted_df['score'] = unadjusted_df.apply(lambda x: 1.0 if x.name in unadjusted_df.head(top_k).index.tolist() else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Aequitas - Before and After"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the original score, without post-hoc adjustment for equity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_unadjusted, _ = g.get_crosstabs(unadjusted_df[['score', 'label_value', protected_attribute_col]].copy())\n",
    "bdf_unadjusted = b.get_disparity_predefined_groups(xtab_unadjusted, original_df=unadjusted_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_unadjusted, metrics, protected_attribute_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the score, with post-hoc disparity mitigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Group()\n",
    "b = Bias()\n",
    "\n",
    "xtab_mitigated, _ = g.get_crosstabs(mitigated_df[['score', 'label_value', protected_attribute_col]].copy())\n",
    "bdf_mitigated = b.get_disparity_predefined_groups(xtab_mitigated, original_df=mitigated_df, ref_groups_dict=protected_attribute_ref_group)\n",
    "\n",
    "ap.disparities_metrics(bdf_mitigated, metrics, protected_attribute_col, fairness_threshold=disparity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it looks the post-hoc adjustments have actually manage to mitigate the existing disparity pretty well (perhaps even over-shooting somewhat, though still within our fairness threshold of 1.3).\n",
    "\n",
    "However, the natural question here is where there is a fairness-accuracy trade-off here: What cost did we incur in terms of model performance, that is overall precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before: Precision of the original, unadjusted score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unadjusted_df.loc[unadjusted_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After: Precision of the new, disparity-mitigated score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_df.loc[mitigated_df['score']==1]['label_value'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, we actually don't seem to see any trade-off with the disparity mitigation here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to the model selection (tradeoff) graph\n",
    "\n",
    "Finally, let's look at how this new option stacks up against what we plotted in our model selection process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigated_precision = mitigated_df.loc[mitigated_df['score']==1]['label_value'].mean()\n",
    "plot_configs = {\n",
    "    'evals_df':evals_df, \n",
    "    'aequitas_df':aequitas_df,\n",
    "    'attr_col':'poverty_level', \n",
    "    'group_name':'highest',\n",
    "    'performance_col':'model_precision',\n",
    "    'bias_metric':'tpr', \n",
    "    'flip_disparity':True, \n",
    "    'mitigated_tag':'Equalizing Recall',\n",
    "    'mitigated_bdf':bdf_mitigated, \n",
    "    'mitigated_performance':mitigated_precision, \n",
    "    'ylim':None\n",
    "}\n",
    "create_scatter_disparity_performance(**plot_configs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
